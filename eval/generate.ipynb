{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2dfa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74a2350319e400291c285e88f47a1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "# model_name = \"./models/Qwen2.5-VL-7B-MUs1\"\n",
    "model_name = \"../../LLaMA-Factory/saves/Qwen2.5-VL-7B-Instruct/freeze/train_2025-04-14\"\n",
    "device = \"cuda:1\"\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a04c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# 将字符串存入json文件\n",
    "def save_data_to_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 从json文件读取字符串\n",
    "def read_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_data_from_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def save_data_to_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            line = json.dumps(item, ensure_ascii=False)\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "\n",
    "def extract_think_content(text: str):\n",
    "    pattern = r'<think>(.*?)</think>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def extract_answer_content(text: str):\n",
    "    pattern = r'<answer>(.*?)</answer>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def messages_generate(system_prompt: str, image_list: list, prompt: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "    content = []\n",
    "    for image_path in image_list:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "            }\n",
    "        )\n",
    "    content.append({\"type\": \"text\", \"text\": prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def inference(messages: list, device: str, text=None, temperature=0.6, top_p=0.95, max_new_tokens=512):\n",
    "\n",
    "\n",
    "    # Preparation for inference\n",
    "    if text is None:\n",
    "        text = processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0], text\n",
    "\n",
    "\n",
    "\n",
    "def split_thinking_answer(output_text):\n",
    "    think_trace = extract_think_content(output_text)\n",
    "    answer = extract_answer_content(output_text)\n",
    "    if think_trace == []:\n",
    "        think_trace = \"\"\n",
    "    else:\n",
    "        think_trace = think_trace[0]\n",
    "    if answer == []:\n",
    "        answer = \"\"\n",
    "    else:\n",
    "        answer = answer[0]\n",
    "    return think_trace, answer\n",
    "    \n",
    "# 测试示例\n",
    "# system_prompt = \"你是一名人工智能助手，专门研究超声医学领域。你收到了一个超声选择题，请给出你的思考过程，并放在<think>思考过程</think>标签内，只输出一个选项，把选项答案放在<answer>选项</answer>内。\"\n",
    "# image_list = [\n",
    "#     \"../../Udata0328/images/238564_1.jpeg\",\n",
    "#     \"../../Udata0328/images/238564_2.jpeg\"\n",
    "# ]\n",
    "# prompt = \"肝脏在这次超声检查中的形态是怎样的？A: 缩小 B: 正常 C: 稍饱满 D: 增大\\n<image>\\n<image>\"\n",
    "# device = \"cuda:0\"\n",
    "# messages = messages_generate(system_prompt=system_prompt, image_list=image_list, prompt=prompt)\n",
    "# temperature=0.6\n",
    "# top_p=0.95\n",
    "# wait = '等等，'\n",
    "# output_text, text = inference(messages=messages, device=device, temperature=temperature, top_p=top_p)\n",
    "# budget_forcing_text = text\n",
    "# for i in range(1):\n",
    "#     think_trace = extract_think_content(output_text)\n",
    "#     answer = extract_answer_content(output_text)\n",
    "#     print(f'think trace:\\n{think_trace[0]}\\n')\n",
    "#     print(f'answer:\\n{answer[0]}\\n')\n",
    "#     budget_forcing_text = budget_forcing_text + '<think>' + think_trace[0] + wait\n",
    "#     output_text, _ = inference(messages=messages, text=budget_forcing_text, device=device, temperature=temperature, top_p=top_p)\n",
    "#     output_text = '<think>' + think_trace[0] + wait + output_text\n",
    "\n",
    "# think_trace = extract_think_content(output_text)\n",
    "# answer = extract_answer_content(output_text)\n",
    "# print(f'think trace:\\n{think_trace[0]}\\n')\n",
    "# print(f'answer:\\n{answer[0]}\\n')\n",
    "# print(f'output_text:\\n{output_text}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 25/341 [09:09<2:05:03, 23.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 17.5 | Total number of questions: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 50/341 [18:38<1:46:16, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 36.0 | Total number of questions: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 75/341 [28:38<1:41:49, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 56.0 | Total number of questions: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 100/341 [39:54<1:31:20, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 72.0 | Total number of questions: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 125/341 [49:29<1:24:42, 23.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 91.5 | Total number of questions: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 150/341 [58:51<1:17:52, 24.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 108.5 | Total number of questions: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 175/341 [1:08:02<41:15, 14.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 129.75 | Total number of questions: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 200/341 [1:13:47<34:11, 14.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 147.75 | Total number of questions: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 225/341 [1:19:34<27:02, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 169.25 | Total number of questions: 225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 250/341 [1:25:49<20:29, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 191.5 | Total number of questions: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 275/341 [1:32:12<15:20, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 211.75 | Total number of questions: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 300/341 [1:38:22<09:28, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 230.75 | Total number of questions: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 325/341 [1:45:08<04:45, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 252.25 | Total number of questions: 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [1:48:43<00:00, 19.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_path = \"/home/wangsj/learn/VSCodeDataProcess/Udata0328/test-data/Reasoning_test_CSYXLCSJJC_filtered.json\"\n",
    "output_path = \"Pass@1_Reasoning_test_CSYXLCSJJC_filtered_k4t6p7l96.jsonl\"\n",
    "image_path_pre = \"/home/wangsj/learn/VSCodeDataProcess/Udata0328/\"\n",
    "system_prompt = \"你是一名人工智能助手，专门研究超声医学领域。你收到了一个超声选择题，请给出你的思考过程，并放在<think>思考过程</think>标签内，只输出一个选项，把选项答案放在<answer>选项</answer>内。\"\n",
    "image_list = [\n",
    "    \"../../Udata0328/images/238564_1.jpeg\",\n",
    "    \"../../Udata0328/images/238564_2.jpeg\"\n",
    "]\n",
    "prompt = \"肝脏在这次超声检查中的形态是怎样的？A: 缩小 B: 正常 C: 稍饱满 D: 增大\\n<image>\\n<image>\"\n",
    "device = \"cuda:1\"\n",
    "pass_1 = 4\n",
    "messages = messages_generate(system_prompt=system_prompt, image_list=image_list, prompt=prompt)\n",
    "temperature=0.6\n",
    "top_p=0.7\n",
    "budget_len = 96\n",
    "max_new_tokens_tmp=1024\n",
    "wait = '等等，'\n",
    "num_ignore = 2\n",
    "\n",
    "sum = 0.0\n",
    "num = 0\n",
    "tmp = {}\n",
    "with open(test_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    for mcq in tqdm(data):\n",
    "        num = num + 1\n",
    "        score = 0.0\n",
    "        prompt = mcq[\"conversations\"][1][\"value\"]\n",
    "        image_list = []\n",
    "        if \"images\" in mcq.keys():\n",
    "            for image_path in mcq[\"images\"]:\n",
    "                image_list.append(f\"{image_path_pre}{image_path}\")\n",
    "        prompt = prompt.replace(\"阴道\", \"yindao\")\n",
    "        messages = messages_generate(system_prompt=system_prompt, image_list=image_list, prompt=prompt)\n",
    "        pass_nlist = []\n",
    "        label_answer = extract_answer_content(mcq[\"conversations\"][2][\"value\"])[0]\n",
    "        for step in range(pass_1):\n",
    "\n",
    "            output_text, text = inference(messages=messages, device=device, temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens_tmp)\n",
    "            budget_forcing_text = text\n",
    "            think_trace, answer = split_thinking_answer(output_text)\n",
    "            num_tmp = 0\n",
    "            while len(think_trace) < budget_len:\n",
    "                num_tmp += 1\n",
    "                tmp_text = budget_forcing_text + '<think>' + think_trace + wait\n",
    "                output_text, _ = inference(messages=messages, text=tmp_text, device=device, temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens_tmp)\n",
    "                output_text = '<think>' + think_trace + wait + output_text\n",
    "                think_trace, answer = split_thinking_answer(output_text)\n",
    "                if num_tmp > num_ignore:\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "            tmp_text = budget_forcing_text + '<think>' + think_trace + '</'\n",
    "            output_text, _ = inference(messages=messages, text=tmp_text, device=device, temperature=temperature, top_p=top_p, max_new_tokens=4096)\n",
    "            output_text =  '<think>' + think_trace + '</' + output_text\n",
    "            think_trace, answer = split_thinking_answer(output_text)\n",
    "            pass_nlist.append(output_text)\n",
    "\n",
    "            if answer == label_answer:\n",
    "                score = score + 1\n",
    "        \n",
    "        score = score / pass_1\n",
    "        tmp[\"prompt\"] = prompt\n",
    "        tmp[\"predict\"] = output_text\n",
    "        tmp[\"label\"] = mcq[\"conversations\"][2][\"value\"]\n",
    "        tmp[\"score\"] = score\n",
    "        tmp[\"pass_nlist\"] = pass_nlist\n",
    "        sum = sum + score\n",
    "            \n",
    "        if num % 25 == 0:\n",
    "            print(f\"Scores: {sum} | Total number of questions: {num}\")\n",
    "        \n",
    "        with open(output_path, \"a\") as f:\n",
    "            f.write(json.dumps(tmp, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "with open(output_path.split('.')[0] + \".txt\", \"w\") as f:\n",
    "    f.write(f\"Scores: {sum}\\n\")\n",
    "    f.write(f\"Total number of questions: {num}\\n\")\n",
    "    f.write(f\"Accuracy: {sum / num:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758061b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
